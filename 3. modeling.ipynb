{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, roc_curve, roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "import pandas as pd\n",
    "import copy\n",
    "import hashlib\n",
    "import os\n",
    "\n",
    "import src.util as util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = util.load_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_feng(params: dict) -> pd.DataFrame:\n",
    "    # Load train set\n",
    "    x_train = util.pickle_load(params[\"train_feng_set_path\"][0])\n",
    "    y_train = util.pickle_load(params[\"train_feng_set_path\"][1])\n",
    "\n",
    "    return x_train, y_train\n",
    "\n",
    "def load_valid_feng(params: dict) -> pd.DataFrame:\n",
    "    # Load valid set\n",
    "    x_valid = util.pickle_load(params[\"valid_feng_set_path\"][0])\n",
    "    y_valid = util.pickle_load(params[\"valid_feng_set_path\"][1])\n",
    "\n",
    "    return x_valid, y_valid\n",
    "\n",
    "def load_test_feng(params: dict) -> pd.DataFrame:\n",
    "    # Load test set\n",
    "    x_test = util.pickle_load(params[\"test_feng_set_path\"][0])\n",
    "    y_test = util.pickle_load(params[\"test_feng_set_path\"][1])\n",
    "\n",
    "    return x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(params: dict) -> pd.DataFrame:\n",
    "    # Debug message\n",
    "    util.print_debug(\"Loading dataset.\")\n",
    "\n",
    "    # Load train set\n",
    "    x_train, y_train = load_train_feng(params)\n",
    "\n",
    "    # Load valid set\n",
    "    x_valid, y_valid = load_valid_feng(params)\n",
    "\n",
    "    # Load test set\n",
    "    x_test, y_test = load_test_feng(params)\n",
    "\n",
    "    # Debug message\n",
    "    util.print_debug(\"Dataset loaded.\")\n",
    "\n",
    "    # Return the dataset\n",
    "    return x_train, y_train, x_valid, y_valid, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Training Log Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_log_template() -> dict:\n",
    "    # Debug message\n",
    "    util.print_debug(\"Creating training log template.\")\n",
    "    \n",
    "    # Template of training log\n",
    "    logger = {\n",
    "        \"model_name\" : [],\n",
    "        \"model_uid\" : [],\n",
    "        \"training_time\" : [],\n",
    "        \"training_date\" : [],\n",
    "        \"performance\" : [],\n",
    "        \"r2_score\" : [],\n",
    "        \"data_configurations\" : [],\n",
    "    }\n",
    "\n",
    "    # Debug message\n",
    "    util.print_debug(\"Training log template created.\")\n",
    "\n",
    "    # Return training log template\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_log_updater(current_log: dict, params: dict) -> list:\n",
    "    # Create copy of current log\n",
    "    current_log = copy.deepcopy(current_log)\n",
    "\n",
    "    # Path for training log file\n",
    "    log_path = params[\"training_log_path\"]\n",
    "\n",
    "    # Try to load training log file\n",
    "    try:\n",
    "        with open(log_path, \"r\") as file:\n",
    "            last_log = json.load(file)\n",
    "        file.close()\n",
    "\n",
    "    # If file not found, create a new one\n",
    "    except FileNotFoundError as fe:\n",
    "        with open(log_path, \"w\") as file:\n",
    "            file.write(\"[]\")\n",
    "        file.close()\n",
    "\n",
    "        with open(log_path, \"r\") as file:\n",
    "            last_log = json.load(file)\n",
    "        file.close()\n",
    "    \n",
    "    # Add current log to previous log\n",
    "    last_log.append(current_log)\n",
    "\n",
    "    # Save updated log\n",
    "    with open(log_path, \"w\") as file:\n",
    "        json.dump(last_log, file)\n",
    "        file.close()\n",
    "\n",
    "    # Return log\n",
    "    return last_log"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Create Model Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_object(params: dict) -> list:\n",
    "    # Debug message\n",
    "    util.print_debug(\"Creating model objects.\")\n",
    "\n",
    "    # Create model objects\n",
    "    lr = LinearRegression()\n",
    "    rfr = RandomForestRegressor()\n",
    "    dct = DecisionTreeRegressor()\n",
    "\n",
    "\n",
    "    # Create list of model\n",
    "    list_of_model = [\n",
    "        { \"model_name\": lr.__class__.__name__, \"model_object\": lr, \"model_uid\": \"\"},\n",
    "        { \"model_name\": rfr.__class__.__name__, \"model_object\": rfr, \"model_uid\": \"\"},\n",
    "        { \"model_name\": dct.__class__.__name__, \"model_object\": dct, \"model_uid\": \"\"},\n",
    "       \n",
    "    ]\n",
    "\n",
    "    # Debug message\n",
    "    util.print_debug(\"Model objects created.\")\n",
    "\n",
    "    # Return the list of model\n",
    "    return list_of_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Training Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(configuration_model: str, params: dict, hyperparams_model: list = None):\n",
    "    # Load dataset\n",
    "    x_train, y_train, \\\n",
    "    x_valid, y_valid, \\\n",
    "    x_test, y_test = load_dataset(params)\n",
    "\n",
    "    # Variabel to store trained models\n",
    "    list_of_trained_model = dict()\n",
    "\n",
    "    # Create log template\n",
    "    training_log = training_log_template()\n",
    "\n",
    "    # Training for every data configuration\n",
    "    for config_data in x_train:\n",
    "        # Debug message\n",
    "        util.print_debug(\"Training model based on configuration data: {}\".format(config_data))\n",
    "\n",
    "        # Create model objects\n",
    "        if hyperparams_model == None:\n",
    "            list_of_model = create_model_object(params)\n",
    "        else:\n",
    "            list_of_model = copy.deepcopy(hyperparams_model)\n",
    "\n",
    "        # Variabel to store tained model\n",
    "        trained_model = list()\n",
    "\n",
    "        # Load train data based on its configuration\n",
    "        x_train_data = x_train[config_data]\n",
    "        y_train_data = y_train[config_data]\n",
    "\n",
    "        # Train each model by current dataset configuration\n",
    "        for model in list_of_model:\n",
    "            # Debug message\n",
    "            util.print_debug(\"Training model: {}\".format(model[\"model_name\"]))\n",
    "\n",
    "            # Training\n",
    "            training_time = util.time_stamp()\n",
    "            model[\"model_object\"].fit(x_train_data, y_train_data)\n",
    "            training_time = (util.time_stamp() - training_time).total_seconds()\n",
    "\n",
    "            # Debug message\n",
    "            util.print_debug(\"Evaluating model: {}\".format(model[\"model_name\"]))\n",
    "\n",
    "            # Evaluation\n",
    "            y_predict = model[\"model_object\"].predict(x_valid)\n",
    "            #performance = classification_report(y_valid, y_predict, output_dict = True)\n",
    "            performance = mean_squared_error(y_valid, y_predict)\n",
    "            r2 = r2_score(y_valid, y_predict)\n",
    "\n",
    "            # Debug message\n",
    "            util.print_debug(\"Logging: {}\".format(model[\"model_name\"]))\n",
    "\n",
    "            # Create UID\n",
    "            uid = hashlib.md5(str(training_time).encode()).hexdigest()\n",
    "\n",
    "            # Assign model's UID\n",
    "            model[\"model_uid\"] = uid\n",
    "\n",
    "            # Create training log data\n",
    "            training_log[\"model_name\"].append(\"{}-{}\".format(configuration_model, model[\"model_name\"]))\n",
    "            training_log[\"model_uid\"].append(uid)\n",
    "            training_log[\"training_time\"].append(training_time)\n",
    "            training_log[\"training_date\"].append(util.time_stamp())\n",
    "            training_log[\"performance\"].append(performance)\n",
    "            training_log[\"r2_score\"].append(r2)\n",
    "            training_log[\"data_configurations\"].append(config_data)\n",
    "\n",
    "            # Collect current trained model\n",
    "            trained_model.append(copy.deepcopy(model))\n",
    "\n",
    "            # Debug message\n",
    "            util.print_debug(\"Model {} has been trained for configuration data {}.\".format(model[\"model_name\"], config_data))\n",
    "        \n",
    "        # Collect current trained list of model\n",
    "        list_of_trained_model[config_data] = copy.deepcopy(trained_model)\n",
    "    \n",
    "    # Debug message\n",
    "    util.print_debug(\"All combination models and configuration data has been trained.\")\n",
    "    \n",
    "    # Return list trained model\n",
    "    return list_of_trained_model, training_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-15 14:14:39.883328 Loading dataset.\n",
      "2023-04-15 14:14:39.887686 Dataset loaded.\n",
      "2023-04-15 14:14:39.887686 Creating training log template.\n",
      "2023-04-15 14:14:39.887686 Training log template created.\n",
      "2023-04-15 14:14:39.887686 Training model based on configuration data: Undersampling\n",
      "2023-04-15 14:14:39.887686 Creating model objects.\n",
      "2023-04-15 14:14:39.887686 Model objects created.\n",
      "2023-04-15 14:14:39.887686 Training model: LinearRegression\n",
      "2023-04-15 14:14:39.893925 Evaluating model: LinearRegression\n",
      "2023-04-15 14:14:39.894431 Logging: LinearRegression\n",
      "2023-04-15 14:14:39.895434 Model LinearRegression has been trained for configuration data Undersampling.\n",
      "2023-04-15 14:14:39.895434 Training model: RandomForestRegressor\n",
      "2023-04-15 14:14:39.985887 Evaluating model: RandomForestRegressor\n",
      "2023-04-15 14:14:39.993894 Logging: RandomForestRegressor\n",
      "2023-04-15 14:14:39.999887 Model RandomForestRegressor has been trained for configuration data Undersampling.\n",
      "2023-04-15 14:14:39.999887 Training model: DecisionTreeRegressor\n",
      "2023-04-15 14:14:40.001887 Evaluating model: DecisionTreeRegressor\n",
      "2023-04-15 14:14:40.002887 Logging: DecisionTreeRegressor\n",
      "2023-04-15 14:14:40.002887 Model DecisionTreeRegressor has been trained for configuration data Undersampling.\n",
      "2023-04-15 14:14:40.007887 Training model based on configuration data: Oversampling\n",
      "2023-04-15 14:14:40.007887 Creating model objects.\n",
      "2023-04-15 14:14:40.007887 Model objects created.\n",
      "2023-04-15 14:14:40.008887 Training model: LinearRegression\n",
      "2023-04-15 14:14:40.010887 Evaluating model: LinearRegression\n",
      "2023-04-15 14:14:40.011887 Logging: LinearRegression\n",
      "2023-04-15 14:14:40.011887 Model LinearRegression has been trained for configuration data Oversampling.\n",
      "2023-04-15 14:14:40.011887 Training model: RandomForestRegressor\n",
      "2023-04-15 14:14:40.248317 Evaluating model: RandomForestRegressor\n",
      "2023-04-15 14:14:40.256768 Logging: RandomForestRegressor\n",
      "2023-04-15 14:14:40.268768 Model RandomForestRegressor has been trained for configuration data Oversampling.\n",
      "2023-04-15 14:14:40.268768 Training model: DecisionTreeRegressor\n",
      "2023-04-15 14:14:40.271768 Evaluating model: DecisionTreeRegressor\n",
      "2023-04-15 14:14:40.272768 Logging: DecisionTreeRegressor\n",
      "2023-04-15 14:14:40.272768 Model DecisionTreeRegressor has been trained for configuration data Oversampling.\n",
      "2023-04-15 14:14:40.282378 Training model based on configuration data: SMOTE\n",
      "2023-04-15 14:14:40.282378 Creating model objects.\n",
      "2023-04-15 14:14:40.282378 Model objects created.\n",
      "2023-04-15 14:14:40.284377 Training model: LinearRegression\n",
      "2023-04-15 14:14:40.286386 Evaluating model: LinearRegression\n",
      "2023-04-15 14:14:40.287425 Logging: LinearRegression\n",
      "2023-04-15 14:14:40.287425 Model LinearRegression has been trained for configuration data SMOTE.\n",
      "2023-04-15 14:14:40.287425 Training model: RandomForestRegressor\n",
      "2023-04-15 14:14:40.603597 Evaluating model: RandomForestRegressor\n",
      "2023-04-15 14:14:40.614143 Logging: RandomForestRegressor\n",
      "2023-04-15 14:14:40.631143 Model RandomForestRegressor has been trained for configuration data SMOTE.\n",
      "2023-04-15 14:14:40.631143 Training model: DecisionTreeRegressor\n",
      "2023-04-15 14:14:40.636143 Evaluating model: DecisionTreeRegressor\n",
      "2023-04-15 14:14:40.637143 Logging: DecisionTreeRegressor\n",
      "2023-04-15 14:14:40.637143 Model DecisionTreeRegressor has been trained for configuration data SMOTE.\n",
      "2023-04-15 14:14:40.654143 All combination models and configuration data has been trained.\n"
     ]
    }
   ],
   "source": [
    "list_of_trained_model, training_log = train_eval(\"Baseline\", params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Choose Best Performance Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_production_model(list_of_model, training_log, params):\n",
    "    # Create copy list of model\n",
    "    list_of_model = copy.deepcopy(list_of_model)\n",
    "    \n",
    "    # Debug message\n",
    "    util.print_debug(\"Choosing model by metrics score.\")\n",
    "\n",
    "    # Create required predefined variabel\n",
    "    curr_production_model = None\n",
    "    prev_production_model = None\n",
    "    production_model_log = None\n",
    "\n",
    "    # Debug message\n",
    "    util.print_debug(\"Converting training log type of data from dict to dataframe.\")\n",
    "\n",
    "    # Convert dictionary to pandas for easy operation\n",
    "    training_log = pd.DataFrame(copy.deepcopy(training_log))\n",
    "\n",
    "    # Debug message\n",
    "    util.print_debug(\"Trying to load previous production model.\")\n",
    "\n",
    "    if os.path.exists(params[\"production_model_path\"]):\n",
    "        try:\n",
    "            # Load the previous production model using JSON\n",
    "            with open(params[\"production_model_path\"], 'r') as f:\n",
    "                serialized_model = f.read()\n",
    "                prev_production_model = json.loads(serialized_model)\n",
    "            util.print_debug(\"Previous production model loaded.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            util.print_debug(\"Failed to load previous production model: {}\".format(str(e)))\n",
    "            prev_production_model = None\n",
    "\n",
    "    else:\n",
    "        util.print_debug(\"No previous production model detected, choosing best model only from current trained model.\")\n",
    "        prev_production_model = None\n",
    "\n",
    "             \n",
    "    # If previous production model detected:\n",
    "    if prev_production_model != None:\n",
    "        # Debug message\n",
    "        util.print_debug(\"Loading validation data.\")\n",
    "        x_valid, y_valid = load_valid_feng(params)\n",
    "        \n",
    "        # Debug message\n",
    "        util.print_debug(\"Checking compatibilty previous production model's input with current train data's features.\")\n",
    "\n",
    "        # Check list features of previous production model and current dataset\n",
    "        production_model_features = set(prev_production_model[\"model_data\"][\"model_object\"].feature_names_in_)\n",
    "        current_dataset_features = set(x_valid.columns)\n",
    "        number_of_different_features = len((production_model_features - current_dataset_features) | (current_dataset_features - production_model_features))\n",
    "\n",
    "        # If feature matched:\n",
    "        if number_of_different_features == 0:\n",
    "            # Debug message\n",
    "            util.print_debug(\"Features compatible.\")\n",
    "\n",
    "            # Debug message\n",
    "            util.print_debug(\"Reassesing previous model performance using current validation data.\")\n",
    "\n",
    "            # Re-predict previous production model to provide valid metrics compared to other current models\n",
    "            y_pred = prev_production_model[\"model_data\"][\"model_object\"].predict(x_valid)\n",
    "\n",
    "            # Re-asses prediction result\n",
    "            eval_res = classification_report(y_valid, y_pred, output_dict = True)\n",
    "\n",
    "            # Debug message\n",
    "            util.print_debug(\"Assessing complete.\")\n",
    "\n",
    "            # Debug message\n",
    "            util.print_debug(\"Storing new metrics data to previous model structure.\")\n",
    "\n",
    "            # Update their performance log\n",
    "            prev_production_model[\"model_log\"][\"performance\"] = eval_res\n",
    "            prev_production_model[\"model_log\"][\"r2_score\"] = eval_res[\"r2-score\"]\n",
    "\n",
    "            # Debug message\n",
    "            util.print_debug(\"Adding previous model data to current training log and list of model\")\n",
    "\n",
    "            # Added previous production model log to current logs to compere who has the greatest r2 score\n",
    "            training_log = pd.concat([training_log, pd.DataFrame([prev_production_model[\"model_log\"]])])\n",
    "\n",
    "            # Added previous production model to current list of models to choose from if it has the greatest r2 score\n",
    "            list_of_model[\"prev_production_model\"] = [copy.deepcopy(prev_production_model[\"model_data\"])]\n",
    "        else:\n",
    "            # To indicate that we are not using previous production model\n",
    "            prev_production_model = None\n",
    "\n",
    "            # Debug message\n",
    "            util.print_debug(\"Different features between production model with current dataset is detected, ignoring production dataset.\")\n",
    "\n",
    "    # Debug message\n",
    "    util.print_debug(\"Sorting training log by r2 macro avg and training time.\")\n",
    "\n",
    "    # Sort training log by MSE, r2 score and training time\n",
    "    best_model_log = training_log.sort_values([\"performance\",\"r2_score\", \"training_time\"], ascending = [True, False, True]).iloc[0]\n",
    "    \n",
    "    # Debug message\n",
    "    util.print_debug(\"Searching model data based on sorted training log.\")\n",
    "\n",
    "    # Get model object with least MSE and greatest r2 score  by using UID\n",
    "    for configuration_data in list_of_model:\n",
    "        for model_data in list_of_model[configuration_data]:\n",
    "            if model_data[\"model_uid\"] == best_model_log[\"model_uid\"]:\n",
    "                curr_production_model = dict()\n",
    "                curr_production_model[\"model_data\"] = copy.deepcopy(model_data)\n",
    "                curr_production_model[\"model_log\"] = copy.deepcopy(best_model_log.to_dict())\n",
    "                curr_production_model[\"model_log\"][\"model_name\"] = \"Production-{}\".format(curr_production_model[\"model_data\"][\"model_name\"])\n",
    "                curr_production_model[\"model_log\"][\"training_date\"] = str(curr_production_model[\"model_log\"][\"training_date\"])\n",
    "                production_model_log = training_log_updater(curr_production_model[\"model_log\"], params)\n",
    "                break\n",
    "    \n",
    "    # In case UID not found\n",
    "    if curr_production_model == None:\n",
    "        raise RuntimeError(\"The best model not found in your list of model.\")\n",
    "    \n",
    "    # Debug message\n",
    "    util.print_debug(\"Model chosen.\")\n",
    "\n",
    "    # Dump chosen production model\n",
    "    util.pickle_dump(curr_production_model, params[\"production_model_path\"])\n",
    "    \n",
    "    # Return current chosen production model, log of production models and current training log\n",
    "    return curr_production_model, production_model_log, training_log\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-15 14:15:14.815538 Choosing model by metrics score.\n",
      "2023-04-15 14:15:14.815538 Converting training log type of data from dict to dataframe.\n",
      "2023-04-15 14:15:14.816538 Trying to load previous production model.\n",
      "2023-04-15 14:15:14.825703 Failed to load previous production model: 'charmap' codec can't decode byte 0x81 in position 114: character maps to <undefined>\n",
      "2023-04-15 14:15:14.825703 Sorting training log by r2 macro avg and training time.\n",
      "2023-04-15 14:15:14.830002 Searching model data based on sorted training log.\n",
      "2023-04-15 14:15:14.847929 Model chosen.\n"
     ]
    }
   ],
   "source": [
    "model, production_model_log, training_logs = get_production_model(list_of_trained_model, training_log, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dist_params(model_name: str) -> dict:\n",
    "    # Define models paramteres\n",
    "\n",
    "\n",
    "    dist_params_lr = {\n",
    "        'fit_intercept': [True, False],\n",
    "        'normalize': [True, False],\n",
    "        'copy_X': [True, False]\n",
    "    }\n",
    "    \n",
    "    dist_params_rfr = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [5, 10, 15],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['auto', 'sqrt', 'log2']\n",
    "    }\n",
    "    \n",
    "    dist_params_dct = {\n",
    "        'max_depth': [2, 4, 6, 8],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['auto', 'sqrt', 'log2']\n",
    "    }\n",
    "\n",
    "\n",
    "    # Make all models parameters in to one\n",
    "    dist_params = {\n",
    "        \"DecisionTreeRegressor\": dist_params_dct,\n",
    "        \"RandomForestRegressor\": dist_params_rfr,\n",
    "        \"LinearRegression\": dist_params_lr\n",
    "        \n",
    "    }\n",
    "\n",
    "    # Return distribution of model parameters\n",
    "    return dist_params[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_params_tuning(model: dict) -> list:\n",
    "    # Create copy of current best baseline model\n",
    "    model = copy.deepcopy(model)\n",
    "\n",
    "    # Create model's parameter distribution\n",
    "    dist_params = create_dist_params(model[\"model_data\"][\"model_name\"])\n",
    "\n",
    "    # Create model object\n",
    "    model_rsc = RandomizedSearchCV(model[\"model_data\"][\"model_object\"], dist_params, n_jobs = -1)\n",
    "    model_data = {\n",
    "        \"model_name\": model[\"model_data\"][\"model_name\"],\n",
    "        \"model_object\": model_rsc,\n",
    "        \"model_uid\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Return model object\n",
    "    return [model_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-15 14:19:08.330910 Loading dataset.\n",
      "2023-04-15 14:19:08.334950 Dataset loaded.\n",
      "2023-04-15 14:19:08.334950 Creating training log template.\n",
      "2023-04-15 14:19:08.334950 Training log template created.\n",
      "2023-04-15 14:19:08.334950 Training model based on configuration data: Undersampling\n",
      "2023-04-15 14:19:08.348914 Training model: RandomForestRegressor\n",
      "2023-04-15 14:20:28.326864 Evaluating model: RandomForestRegressor\n",
      "2023-04-15 14:20:28.341267 Logging: RandomForestRegressor\n",
      "2023-04-15 14:20:28.370095 Model RandomForestRegressor has been trained for configuration data Undersampling.\n",
      "2023-04-15 14:20:28.394446 Training model based on configuration data: Oversampling\n",
      "2023-04-15 14:20:28.412117 Training model: RandomForestRegressor\n",
      "2023-04-15 14:20:29.950587 Evaluating model: RandomForestRegressor\n",
      "2023-04-15 14:20:29.956588 Logging: RandomForestRegressor\n",
      "2023-04-15 14:20:29.979664 Model RandomForestRegressor has been trained for configuration data Oversampling.\n",
      "2023-04-15 14:20:30.000665 Training model based on configuration data: SMOTE\n",
      "2023-04-15 14:20:30.017587 Training model: RandomForestRegressor\n",
      "2023-04-15 14:20:32.349963 Evaluating model: RandomForestRegressor\n",
      "2023-04-15 14:20:32.356963 Logging: RandomForestRegressor\n",
      "2023-04-15 14:20:32.380962 Model RandomForestRegressor has been trained for configuration data SMOTE.\n",
      "2023-04-15 14:20:32.401072 All combination models and configuration data has been trained.\n"
     ]
    }
   ],
   "source": [
    "list_of_trained_model, training_log = train_eval(\"Hyperparams_Tuning\", params, hyper_params_tuning(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-15 14:20:50.309543 Choosing model by metrics score.\n",
      "2023-04-15 14:20:50.310545 Converting training log type of data from dict to dataframe.\n",
      "2023-04-15 14:20:50.311555 Trying to load previous production model.\n",
      "2023-04-15 14:20:50.324551 Failed to load previous production model: 'charmap' codec can't decode byte 0x81 in position 114: character maps to <undefined>\n",
      "2023-04-15 14:20:50.326540 Sorting training log by r2 macro avg and training time.\n",
      "2023-04-15 14:20:50.328506 Searching model data based on sorted training log.\n",
      "2023-04-15 14:20:50.348604 Model chosen.\n"
     ]
    }
   ],
   "source": [
    "model, production_model_log, training_logs = get_production_model(list_of_trained_model, training_log, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Take a Look at Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid, y_valid = load_valid_feng(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model[\"model_data\"][\"model_object\"].predict(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: (1642     0.831966\n553      6.240909\n1189     4.293156\n1145    15.017377\n965      1.396418\n          ...    \n992      2.426187\n1433     1.467594\n1601    10.622030\n1277     3.062086\n1950     2.127503\nName: EFConsPerCap, Length: 345, dtype: float64, array([ 1.64189472,  6.30147401,  2.13413277,  5.14406232,  2.30807866,\n        3.97771209, 10.58480909,  1.6624049 ,  1.98284472,  0.92135653,\n        1.4679926 ,  2.45073189,  7.21479844,  3.33425095,  5.71966222,\n        9.78940817,  1.11951202,  1.60512817,  1.66998242,  1.44370112,\n        4.89263718,  2.4995924 ,  4.85558378,  1.80765429,  1.64006374,\n        7.52041564,  2.13143587,  6.5073347 ,  2.52077334,  1.6643239 ,\n        0.91931713,  1.60526201,  2.76069943,  8.1447046 ,  3.7297187 ,\n        1.14433117,  3.12229377,  0.58117691,  0.90968781,  2.1424704 ,\n        0.97564222,  5.70250929,  2.38329385,  4.69143111,  1.31417459,\n        1.1445893 ,  1.94814869,  6.34272353,  0.96978678,  6.19680845,\n        1.43293805,  1.75130194,  0.6802435 ,  3.02119674,  3.73394675,\n        0.9657133 ,  0.69384739,  1.12524062,  1.36270913,  1.20799645,\n        3.56560493,  2.54746497,  1.21229285,  7.42299575,  5.45432758,\n        2.41965436,  2.7170888 ,  0.83083389,  5.78381753,  0.94704813,\n        3.12229377,  1.43055619,  7.29226105,  1.18172799,  1.96322841,\n        2.94765816,  1.76670671,  3.18672042,  5.72795352,  3.28062827,\n        4.89263718,  1.34133518,  2.91530623,  7.08622734,  2.50640465,\n        0.89712635,  2.61260119,  5.98304428,  1.9067665 ,  5.7723028 ,\n        8.83224593,  2.61889082, 10.80778678,  1.36197237,  6.19680845,\n        5.31277088,  1.49084231,  5.76263881,  5.52742511,  1.6111836 ,\n        1.11640929,  1.37888393,  1.6111836 ,  5.59475372,  1.11147778,\n        2.88019331,  2.12246103,  1.25204235,  5.14406232,  5.45432758,\n        1.62064134,  1.64078332,  1.20799645,  2.03998453,  5.76263881,\n       11.8288133 ,  7.55707027,  1.69359952,  1.25442135,  2.14367889,\n        1.11147778,  5.48919939,  1.97295378,  6.27619036,  0.9281833 ,\n        3.33301411,  5.86444014,  6.75139305,  1.89990599,  3.02844556,\n        2.77010611,  2.34057143,  2.07281282,  2.59518853,  2.20243836,\n        5.39358172,  2.1909231 ,  2.24835585,  1.97412751,  2.90351501,\n        2.60762743,  9.94666063,  7.55707027,  5.59780066,  1.16357161,\n        2.38457425,  1.03521491,  1.20503413,  6.28023204,  4.14982368,\n        1.21229285,  2.12246103,  2.48514903,  5.69180078,  2.24070607,\n        2.35189993,  1.16305793,  4.43365022,  0.9663293 ,  0.97564222,\n        0.91495166,  1.9802765 ,  2.19728638,  5.72795352,  0.96978678,\n        2.68067988,  1.43055619,  5.87647502,  5.31782346,  0.85173862,\n        5.9316496 ,  7.5934345 ,  0.8341973 ,  2.84624188,  8.1447046 ,\n       10.84396207,  0.83067873,  1.63990185,  1.84977657,  3.59072849,\n        6.73997544,  9.33085207,  1.51840554,  6.61181675,  7.5934345 ,\n        2.74717187,  4.69143111,  8.68086327,  2.47756836,  2.05871168,\n        1.02263218,  2.3097027 ,  1.62441698,  1.83559633,  5.32364474,\n        0.90968781,  1.49567921,  5.45737827,  8.98259049,  5.90036015,\n        1.56713123,  3.92289503,  2.60762743,  2.38329385,  6.327175  ,\n        1.39889396,  1.66070906,  5.13418254,  1.7834883 ,  1.08769287,\n        5.97966202,  3.83763027,  1.31417459,  5.48919939,  2.65381917,\n        2.19431865,  1.32117837,  4.89263718, 10.49682228,  5.75252438,\n        3.20757363,  6.80952214,  1.39413671,  5.22748967,  1.95288488,\n        5.72795352,  1.18172799,  1.31417459,  1.00395551,  4.13127547,\n        2.38934384,  2.23002633,  5.65140498,  2.17232943, 11.90042813,\n        2.06572297,  2.74717187,  7.52041564,  0.70298976,  3.09369905,\n       10.67942692,  2.12460872,  2.28838324,  6.95655408,  5.6281038 ,\n        3.71046377,  4.85558378,  1.67284995,  2.36766112,  5.72198734,\n        1.6485111 ,  1.0528694 ,  5.59780066,  1.82780242,  3.15333512,\n        1.31603216,  1.12534064, 10.0324481 ,  2.04011644,  7.19700911,\n        6.5073347 ,  0.97283367,  3.51100533,  1.26057808,  0.83083389,\n        1.60512817,  1.4596551 ,  3.1123878 ,  4.23611949,  5.80695778,\n        1.27110699,  2.02993837,  7.99406089,  2.13898415,  2.4216709 ,\n        7.53575258,  2.12161535,  1.14433117,  1.54586856,  3.73394675,\n        3.97304364,  2.0417353 ,  3.92289503,  3.612034  ,  2.83557762,\n        2.15106204,  5.31782346,  1.03941309,  4.37777514,  1.27110699,\n        5.85840447,  8.52619227,  1.78044315,  1.97604936,  1.21152569,\n        7.19700911,  5.97966202,  6.73997544,  1.18571055,  0.90968781,\n        3.13488386,  6.73997544,  1.66853846,  4.65161162,  1.00261982,\n        2.94315455,  0.90425415,  1.31603216,  4.10054101,  2.28349677,\n        3.22644849,  2.84594303, 13.57174038,  8.37677193,  4.89263718,\n        1.03941309,  2.77655961,  7.93585119,  5.13418254,  1.07672724,\n        2.15189212,  6.51315457,  1.60526201,  1.58542369,  2.55081207,\n        2.13413277,  0.89588823,  1.7381054 ,  2.8446053 ,  1.57544902,\n        5.52742511,  1.20799645,  1.95714038,  3.34209825,  5.84498092,\n        0.69384739,  0.96684057,  1.70556371,  5.41951795,  7.66792277,\n        2.20527088,  1.70960711, 10.67929376,  2.05335728,  2.2430925 ]))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ConfusionMatrixDisplay\u001b[39m.\u001b[39;49mfrom_predictions(y_valid, y_pred)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_plot\\confusion_matrix.py:437\u001b[0m, in \u001b[0;36mConfusionMatrixDisplay.from_predictions\u001b[1;34m(cls, y_true, y_pred, labels, sample_weight, normalize, display_labels, include_values, xticks_rotation, values_format, cmap, ax, colorbar, im_kw)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[39mif\u001b[39;00m display_labels \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    436\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 437\u001b[0m         display_labels \u001b[39m=\u001b[39m unique_labels(y_true, y_pred)\n\u001b[0;32m    438\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    439\u001b[0m         display_labels \u001b[39m=\u001b[39m labels\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\multiclass.py:103\u001b[0m, in \u001b[0;36munique_labels\u001b[1;34m(*ys)\u001b[0m\n\u001b[0;32m    101\u001b[0m _unique_labels \u001b[39m=\u001b[39m _FN_UNIQUE_LABELS\u001b[39m.\u001b[39mget(label_type, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    102\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _unique_labels:\n\u001b[1;32m--> 103\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnknown label type: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mrepr\u001b[39m(ys))\n\u001b[0;32m    105\u001b[0m ys_labels \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(chain\u001b[39m.\u001b[39mfrom_iterable(_unique_labels(y) \u001b[39mfor\u001b[39;00m y \u001b[39min\u001b[39;00m ys))\n\u001b[0;32m    107\u001b[0m \u001b[39m# Check that we don't mix string type with number type\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: (1642     0.831966\n553      6.240909\n1189     4.293156\n1145    15.017377\n965      1.396418\n          ...    \n992      2.426187\n1433     1.467594\n1601    10.622030\n1277     3.062086\n1950     2.127503\nName: EFConsPerCap, Length: 345, dtype: float64, array([ 1.64189472,  6.30147401,  2.13413277,  5.14406232,  2.30807866,\n        3.97771209, 10.58480909,  1.6624049 ,  1.98284472,  0.92135653,\n        1.4679926 ,  2.45073189,  7.21479844,  3.33425095,  5.71966222,\n        9.78940817,  1.11951202,  1.60512817,  1.66998242,  1.44370112,\n        4.89263718,  2.4995924 ,  4.85558378,  1.80765429,  1.64006374,\n        7.52041564,  2.13143587,  6.5073347 ,  2.52077334,  1.6643239 ,\n        0.91931713,  1.60526201,  2.76069943,  8.1447046 ,  3.7297187 ,\n        1.14433117,  3.12229377,  0.58117691,  0.90968781,  2.1424704 ,\n        0.97564222,  5.70250929,  2.38329385,  4.69143111,  1.31417459,\n        1.1445893 ,  1.94814869,  6.34272353,  0.96978678,  6.19680845,\n        1.43293805,  1.75130194,  0.6802435 ,  3.02119674,  3.73394675,\n        0.9657133 ,  0.69384739,  1.12524062,  1.36270913,  1.20799645,\n        3.56560493,  2.54746497,  1.21229285,  7.42299575,  5.45432758,\n        2.41965436,  2.7170888 ,  0.83083389,  5.78381753,  0.94704813,\n        3.12229377,  1.43055619,  7.29226105,  1.18172799,  1.96322841,\n        2.94765816,  1.76670671,  3.18672042,  5.72795352,  3.28062827,\n        4.89263718,  1.34133518,  2.91530623,  7.08622734,  2.50640465,\n        0.89712635,  2.61260119,  5.98304428,  1.9067665 ,  5.7723028 ,\n        8.83224593,  2.61889082, 10.80778678,  1.36197237,  6.19680845,\n        5.31277088,  1.49084231,  5.76263881,  5.52742511,  1.6111836 ,\n        1.11640929,  1.37888393,  1.6111836 ,  5.59475372,  1.11147778,\n        2.88019331,  2.12246103,  1.25204235,  5.14406232,  5.45432758,\n        1.62064134,  1.64078332,  1.20799645,  2.03998453,  5.76263881,\n       11.8288133 ,  7.55707027,  1.69359952,  1.25442135,  2.14367889,\n        1.11147778,  5.48919939,  1.97295378,  6.27619036,  0.9281833 ,\n        3.33301411,  5.86444014,  6.75139305,  1.89990599,  3.02844556,\n        2.77010611,  2.34057143,  2.07281282,  2.59518853,  2.20243836,\n        5.39358172,  2.1909231 ,  2.24835585,  1.97412751,  2.90351501,\n        2.60762743,  9.94666063,  7.55707027,  5.59780066,  1.16357161,\n        2.38457425,  1.03521491,  1.20503413,  6.28023204,  4.14982368,\n        1.21229285,  2.12246103,  2.48514903,  5.69180078,  2.24070607,\n        2.35189993,  1.16305793,  4.43365022,  0.9663293 ,  0.97564222,\n        0.91495166,  1.9802765 ,  2.19728638,  5.72795352,  0.96978678,\n        2.68067988,  1.43055619,  5.87647502,  5.31782346,  0.85173862,\n        5.9316496 ,  7.5934345 ,  0.8341973 ,  2.84624188,  8.1447046 ,\n       10.84396207,  0.83067873,  1.63990185,  1.84977657,  3.59072849,\n        6.73997544,  9.33085207,  1.51840554,  6.61181675,  7.5934345 ,\n        2.74717187,  4.69143111,  8.68086327,  2.47756836,  2.05871168,\n        1.02263218,  2.3097027 ,  1.62441698,  1.83559633,  5.32364474,\n        0.90968781,  1.49567921,  5.45737827,  8.98259049,  5.90036015,\n        1.56713123,  3.92289503,  2.60762743,  2.38329385,  6.327175  ,\n        1.39889396,  1.66070906,  5.13418254,  1.7834883 ,  1.08769287,\n        5.97966202,  3.83763027,  1.31417459,  5.48919939,  2.65381917,\n        2.19431865,  1.32117837,  4.89263718, 10.49682228,  5.75252438,\n        3.20757363,  6.80952214,  1.39413671,  5.22748967,  1.95288488,\n        5.72795352,  1.18172799,  1.31417459,  1.00395551,  4.13127547,\n        2.38934384,  2.23002633,  5.65140498,  2.17232943, 11.90042813,\n        2.06572297,  2.74717187,  7.52041564,  0.70298976,  3.09369905,\n       10.67942692,  2.12460872,  2.28838324,  6.95655408,  5.6281038 ,\n        3.71046377,  4.85558378,  1.67284995,  2.36766112,  5.72198734,\n        1.6485111 ,  1.0528694 ,  5.59780066,  1.82780242,  3.15333512,\n        1.31603216,  1.12534064, 10.0324481 ,  2.04011644,  7.19700911,\n        6.5073347 ,  0.97283367,  3.51100533,  1.26057808,  0.83083389,\n        1.60512817,  1.4596551 ,  3.1123878 ,  4.23611949,  5.80695778,\n        1.27110699,  2.02993837,  7.99406089,  2.13898415,  2.4216709 ,\n        7.53575258,  2.12161535,  1.14433117,  1.54586856,  3.73394675,\n        3.97304364,  2.0417353 ,  3.92289503,  3.612034  ,  2.83557762,\n        2.15106204,  5.31782346,  1.03941309,  4.37777514,  1.27110699,\n        5.85840447,  8.52619227,  1.78044315,  1.97604936,  1.21152569,\n        7.19700911,  5.97966202,  6.73997544,  1.18571055,  0.90968781,\n        3.13488386,  6.73997544,  1.66853846,  4.65161162,  1.00261982,\n        2.94315455,  0.90425415,  1.31603216,  4.10054101,  2.28349677,\n        3.22644849,  2.84594303, 13.57174038,  8.37677193,  4.89263718,\n        1.03941309,  2.77655961,  7.93585119,  5.13418254,  1.07672724,\n        2.15189212,  6.51315457,  1.60526201,  1.58542369,  2.55081207,\n        2.13413277,  0.89588823,  1.7381054 ,  2.8446053 ,  1.57544902,\n        5.52742511,  1.20799645,  1.95714038,  3.34209825,  5.84498092,\n        0.69384739,  0.96684057,  1.70556371,  5.41951795,  7.66792277,\n        2.20527088,  1.70960711, 10.67929376,  2.05335728,  2.2430925 ]))"
     ]
    }
   ],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.-1"
  },
  "vscode": {
   "interpreter": {
    "hash": "f6b5ba345f865ccab9c1d1dba86e394ca61d67366183ce7e182371ee31a721f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
